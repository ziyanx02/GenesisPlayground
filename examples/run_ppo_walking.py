#!/usr/bin/env python3
"""Example: Train PPO on Genesis Walking environment using Genesis RL."""

import glob
import os
import platform
from datetime import datetime
from pathlib import Path
from typing import Any

import fire
import matplotlib

matplotlib.use("Agg")  # Use non-interactive backend to prevent windows from showing
import matplotlib.pyplot as plt
import numpy as np
import torch
from gs_agent.algos.config.registry import PPO_WALKING_MLP
from gs_agent.algos.ppo import PPO
from gs_agent.runners.config.registry import RUNNER_WALKING_MLP
from gs_agent.runners.onpolicy_runner import OnPolicyRunner
from gs_agent.utils.logger import configure as logger_configure
from gs_agent.utils.policy_loader import load_latest_model
from gs_agent.wrappers.gs_env_wrapper import GenesisEnvWrapper
from gs_env.sim.envs.config.registry import EnvArgsRegistry
import gs_env.sim.envs as gs_envs
from utils import apply_overrides_generic, config_to_yaml, plot_metric_on_axis


def create_gs_env(
    show_viewer: bool = False,
    num_envs: int = 4096,
    device: str = "cuda",
    args: Any = None,
    eval_mode: bool = False,
) -> gs_envs.WalkingEnv:
    """Create gym environment wrapper with optional config overrides."""
    if torch.cuda.is_available() and device == "cuda":
        device_tensor = torch.device("cuda")
    else:
        device_tensor = torch.device("cpu")
    print(f"Using device: {device_tensor}")

    env_class = getattr(gs_envs, args.env_name)

    return env_class(
        args=args,
        num_envs=num_envs,
        show_viewer=show_viewer,
        device=device_tensor,  # type: ignore
        eval_mode=eval_mode,
    )


def _apply_algo_overrides(cfg: Any, overrides: dict[str, Any] | None) -> Any:
    """Deep-apply overrides to PPOArgs (and nested models)."""
    return apply_overrides_generic(cfg, overrides, prefixes=("cfgs.", "ppo.", "algo."))


def _apply_runner_overrides(runner_args: Any, overrides: dict[str, Any] | None) -> Any:
    """Deep-apply overrides to RunnerArgs."""
    return apply_overrides_generic(runner_args, overrides, prefixes=("cfgs.", "runner."))


def create_ppo_runner_from_registry(
    env: gs_envs.WalkingEnv,
    exp_name: str | None = None,
    algo_cfg: Any = None,
    runner_args: Any = None,
) -> OnPolicyRunner:
    """Create PPO runner using configuration from the registry."""
    # Environment setup
    wrapped_env = GenesisEnvWrapper(env, device=env.device)

    # Create PPO algorithm
    ppo = PPO(
        env=wrapped_env,
        cfg=algo_cfg,
        device=wrapped_env.device,
    )

    # Create PPO runner
    if exp_name is not None:
        # Avoid mutating a frozen Pydantic model; create a copied config with updated save_path
        runner_args = RUNNER_WALKING_MLP.model_copy(
            update={"save_path": Path(f"./logs/{exp_name}")}
        )
    runner = OnPolicyRunner(
        algorithm=ppo,
        runner_args=runner_args,
        device=wrapped_env.device,
    )
    return runner


def evaluate_policy(
    exp_name: str,
    show_viewer: bool = False,
    num_ckpt: int | None = None,
    device: str = "cuda",
    env_args: Any = None,
    algo_cfg: Any = None,
) -> None:
    """Evaluate the policy."""
    print("=" * 80)
    print("EVALUATION MODE: Disabling domain randomization, observation noise, and random push")
    print("=" * 80)

    # Disable domain randomization, obs noise, and random push for evaluation
    # Create a copy of env_args with disabled randomization
    env_args = env_args.model_copy(
        update={
            "obs_noises": {},  # Disable observation noise
        }
    )
    # Update robot args to disable domain randomization
    from gs_env.sim.robots.config.schema import DomainRandomizationArgs

    robot_args = env_args.robot_args.model_copy(
        update={
            "dr_args": DomainRandomizationArgs(
                kp_range=(1.0, 1.0),
                kd_range=(1.0, 1.0),
                motor_strength_range=(1.0, 1.0),
                motor_offset_range=(0.0, 0.0),
                friction_range=(1.0, 1.0),
                mass_range=(0.0, 0.0),
                com_displacement_range=(0.0, 0.0),
            )
        }
    )
    env_args = env_args.model_copy(update={"robot_args": robot_args})

    # Find the experiment directory without creating a new runner
    log_pattern = f"logs/{exp_name}/*"
    log_dirs = glob.glob(log_pattern)
    if not log_dirs:
        raise FileNotFoundError(
            f"No experiment directories found matching pattern: {log_pattern}"
        )

    log_dirs.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    exp_dir = log_dirs[0]
    print(f"Loading policy from experiment: {exp_dir}")

    # Load checkpoint - either specific one or latest
    if num_ckpt is not None:
        ckpt_path = Path(exp_dir) / "checkpoints" / f"checkpoint_{num_ckpt:04d}.pt"
        if not ckpt_path.exists():
            raise FileNotFoundError(f"Checkpoint {ckpt_path} not found")
    else:
        ckpt_path = load_latest_model(Path(exp_dir))
        num_ckpt = int(ckpt_path.stem.split("_")[-1])

    print(f"Loading checkpoint: {ckpt_path}")

    # Create environment for evaluation
    env = create_gs_env(
        show_viewer=show_viewer,
        num_envs=1,
        device=device,
        args=env_args,
        eval_mode=True,
    )

    wrapped_env = GenesisEnvWrapper(env, device=env.device)

    # Setup GIF recording if not showing viewer
    gif_path = None
    if not show_viewer:
        # Create gif directory structure
        gif_dir = Path("./gif") / exp_name
        gif_dir.mkdir(parents=True, exist_ok=True)

        gif_path = gif_dir / f"{num_ckpt}.gif"
        print(f"Will save GIF to: {gif_path}")

        # Start rendering
        env.start_rendering()  # type: ignore

    # Create PPO algorithm and load checkpoint
    ppo = PPO(
        env=wrapped_env,
        cfg=algo_cfg,
        device=wrapped_env.device,
    )
    ppo.load(ckpt_path, load_optimizer=False)

    # Get inference policy
    inference_policy = ppo.get_inference_policy()

    print("Starting evaluation...")

    def evaluate() -> None:
        nonlocal wrapped_env, inference_policy, gif_path, show_viewer, num_ckpt, exp_name, env_args
        if show_viewer:
            print("Running endlessly (press Ctrl+C to stop)")
        else:
            print("Running until all environments are done")

        step_count = 0
        total_reward = 0.0

        # For tracking action changes
        upper_body_action_diffs_mean = []
        upper_body_action_diffs_max = []
        lower_body_action_diffs_mean = []
        lower_body_action_diffs_max = []
        upper_body_action_rates_mean = []
        upper_body_action_rates_max = []
        lower_body_action_rates_mean = []
        lower_body_action_rates_max = []
        last_action = None

        # Reset environment
        obs, _ = wrapped_env.get_observations()

        # Create a wrapper that always uses deterministic=True
        class DeterministicWrapper(torch.nn.Module):
            def __init__(self, policy):
                super().__init__()
                self.policy = policy
            
            def forward(self, obs):
                action, _ = self.policy(obs, deterministic=True)
                return action
    
        # Wrap and trace the policy with deterministic=True baked in
        wrapped_policy = DeterministicWrapper(inference_policy)
        inference_policy = torch.jit.trace(wrapped_policy, obs)

        # Save the JIT-traced policy and env_args to deploy folder
        print("Saving JIT-traced policy and env_args for deployment...")

        # Create deploy directory structure
        deploy_dir = Path("./deploy/logs") / exp_name
        deploy_dir.mkdir(parents=True, exist_ok=True)
        
        # Save JIT-traced policy
        jit_policy_path = deploy_dir / f"checkpoint_{num_ckpt}.pt"
        torch.jit.save(inference_policy, str(jit_policy_path))
        print(f"JIT-traced policy saved to: {jit_policy_path}")

        # Save env_args as YAML
        env_args_path = deploy_dir / "env_args.yaml"
        config_to_yaml(env_args, env_args_path)
        print(f"Environment args saved to: {env_args_path}")

        while True:
            if step_count < 100:
                wrapped_env.env.commands[:] = 0.0
            elif step_count < 200:
                wrapped_env.env.commands[:, 0] = 0.0
                wrapped_env.env.commands[:, 2] = 1.0
            else:
                wrapped_env.env.commands[:, 0] = 1.0
                wrapped_env.env.commands[:, 2] = 0.0

            # Get action from policy
            with torch.no_grad():
                action = inference_policy(obs)  # type: ignore[misc]

            action_np = action[0].cpu().numpy()
            # Track action changes for first 500 steps
            if step_count < 500 and step_count > 0:
                dof_pos = wrapped_env.env.dof_pos[0] - wrapped_env.env.robot.default_dof_pos
                scaled_dof_pos = dof_pos.cpu().numpy() / env_args.robot_args.action_scale
                action_diff = np.abs(action_np - scaled_dof_pos) * env_args.robot_args.action_scale
                upper_body_action_diffs_mean.append(np.mean(action_diff[:12]))
                upper_body_action_diffs_max.append(np.max(action_diff[:12]))
                lower_body_action_diffs_mean.append(np.mean(action_diff[12:]))
                lower_body_action_diffs_max.append(np.max(action_diff[12:]))

                action_rate = np.abs(action_np - last_action) * env_args.robot_args.action_scale
                upper_body_action_rates_mean.append(np.mean(action_rate[:12]))
                upper_body_action_rates_max.append(np.max(action_rate[:12]))
                lower_body_action_rates_mean.append(np.mean(action_rate[12:]))
                lower_body_action_rates_max.append(np.max(action_rate[12:]))
            elif step_count == 500:
                print("\nPlotting action differences...")
                steps = np.arange(1, len(upper_body_action_rates_mean) + 1)

                # Create figure with 4 subplots in one column
                fig, axes = plt.subplots(4, 1, figsize=(12, 12))

                # Upper body action rate
                plot_metric_on_axis(
                    axes[0],
                    steps,
                    [upper_body_action_rates_mean, upper_body_action_rates_max],
                    ["Mean", "Max"],
                    "Action Rate (log)",
                    "Upper Body Action Rate",
                    yscale="log",
                )

                # Lower body action rate
                plot_metric_on_axis(
                    axes[1],
                    steps,
                    [lower_body_action_rates_mean, lower_body_action_rates_max],
                    ["Mean", "Max"],
                    "Action Rate (log)",
                    "Lower Body Action Rate",
                    yscale="log",
                )

                # Upper body action diff
                plot_metric_on_axis(
                    axes[2],
                    steps,
                    [upper_body_action_diffs_mean, upper_body_action_diffs_max],
                    ["Mean", "Max"],
                    "Action Diff (log)",
                    "Upper Body Action Diff",
                    yscale="log",
                )

                # Lower body action diff
                plot_metric_on_axis(
                    axes[3],
                    steps,
                    [lower_body_action_diffs_mean, lower_body_action_diffs_max],
                    ["Mean", "Max"],
                    "Action Diff (log)",
                    "Lower Body Action Diff",
                    yscale="log",
                    xlabel="Step",
                )

                # Save plot
                plot_dir = Path("./gif") / exp_name if exp_name else Path("./gif") / "latest"
                plot_dir.mkdir(parents=True, exist_ok=True)

                if num_ckpt is not None:
                    ckpt_num = num_ckpt
                else:
                    ckpt_filename = ckpt_path.stem
                    ckpt_num = ckpt_filename.split("_")[-1] if "_" in ckpt_filename else "latest"

                plot_path = plot_dir / f"{ckpt_num}_action_log.png"
                plt.tight_layout()
                plt.savefig(plot_path, dpi=150)
                plt.close(fig)
                print(f"Action difference plot saved to: {plot_path}")
            last_action = action_np.copy()

            # Step environment
            obs, reward, terminated, truncated, _ = wrapped_env.step(action)
            # print(wrapped_env.env.feet_contact_force[0].cpu().numpy())

            # Accumulate reward
            total_reward += reward.item()
            step_count += 1

            # Print progress
            if step_count % 50 == 0:
                print(f"Step {step_count}, Total reward: {total_reward:.2f}")

            # Check if all environments are done (for non-viewer mode)
            if not show_viewer:
                if terminated.item() or truncated.item():
                    print(f"Episode ended at step {step_count}, Total reward: {total_reward:.2f}")
                    break
            else:
                # For viewer mode, check termination conditions
                if terminated.item() or truncated.item():
                    print(f"Episode ended at step {step_count}, Total reward: {total_reward:.2f}")
                    obs, _ = wrapped_env.get_observations()
                    total_reward = 0.0

        # Stop rendering and save GIF if recording
        if not show_viewer and gif_path is not None:
            print("Stopping rendering and saving GIF...")
            env.stop_rendering(save_gif=True, gif_path=str(gif_path))  # type: ignore
            print(f"GIF saved to: {gif_path}")

        print(f"Evaluation of checkpoint {ckpt_path} completed successfully!")
        print("Final evaluation results:")
        print(f"Total steps: {step_count}")
        print(f"Final reward: {total_reward:.2f}")

    try:
        if platform.system() == "Darwin" and show_viewer:
            import threading

            threading.Thread(target=evaluate).start()
            env.scene.scene.viewer.run()  # type: ignore
        else:
            evaluate()
    except KeyboardInterrupt:
        pass


def train_policy(
    exp_name: str | None = None,
    show_viewer: bool = False,
    num_envs: int = 4096,
    device: str = "cuda",
    use_wandb: bool = True,
    env_args: Any = None,
    algo_cfg: Any = None,
    runner_args: Any = None,
) -> None:
    """Train the policy using PPO."""

    # Create environment
    env = create_gs_env(
        show_viewer=show_viewer,
        num_envs=num_envs,
        device=device,
        args=env_args,
    )

    # Get configuration and runner from registry
    runner = create_ppo_runner_from_registry(
        env,
        exp_name=exp_name,
        algo_cfg=algo_cfg,
        runner_args=runner_args,
    )

    # Set up logging with proper configuration
    if exp_name is not None:
        save_path = Path(f"./logs/{exp_name}")
    else:
        save_path = RUNNER_WALKING_MLP.save_path
    logger_folder = save_path / datetime.now().strftime("%Y%m%d_%H%M%S")
    logger = logger_configure(
        folder=str(logger_folder),
        format_strings=["stdout", "csv", "wandb"],
        entity=None,
        project=None,
        exp_name=exp_name,
        mode="online" if use_wandb and (not show_viewer) else "disabled",
    )

    # Save configuration files to YAML
    print("Saving configuration files to YAML...")
    config_dir = logger_folder / "configs"
    config_dir.mkdir(parents=True, exist_ok=True)
    config_to_yaml(env_args, config_dir / "env_args.yaml")
    config_to_yaml(algo_cfg, config_dir / "algo_cfg.yaml")
    config_to_yaml(runner_args, config_dir / "runner_args.yaml")

    # Train using Runner
    print("Starting training...")

    def train() -> None:
        nonlocal runner, logger
        train_summary_info = {
            "total_time": 0.0,
            "total_episodes": 0,
            "total_steps": 0,
            "final_reward": 0.0,
        }
        train_summary_info = runner.train(metric_logger=logger)
        print("Training completed successfully!")
        print(f"Training completed in {train_summary_info['total_time']:.2f} seconds.")
        print(f"Total episodes: {train_summary_info['total_episodes']}.")
        print(f"Total steps: {train_summary_info['total_steps']}.")
        print(f"Total reward: {train_summary_info['final_reward']:.2f}.")

    try:
        if platform.system() == "Darwin" and show_viewer:
            import threading

            threading.Thread(target=train).start()
            env.scene.scene.viewer.run()  # type: ignore
        else:
            train()
    except KeyboardInterrupt:
        pass


def main(
    num_envs: int = 4096,
    show_viewer: bool = False,
    device: str = "cuda",
    eval: bool = False,
    exp_name: str | None = None,
    num_ckpt: int | None = None,
    use_wandb: bool = True,
    env_name: str = "g1_walk",
    **cfg_overrides: Any,
) -> None:
    """Entry point.

    You can override configs via dot-notation, e.g.:
    --env.reward_args.AngVelZReward=5
    --reward_args.G1BaseHeightPenalty=50
    --algo.lr=1e-4
    --runner.total_iterations=2000
    """
    # Bucket overrides into env / algo / runner
    env_overrides: dict[str, Any] = {}
    algo_overrides: dict[str, Any] = {}
    runner_overrides: dict[str, Any] = {}

    for k, v in cfg_overrides.items():
        if k.startswith("cfgs.env.") or k.startswith("env.") or k.startswith("reward_args."):
            env_overrides[k] = v
            continue
        if k.startswith("cfgs.algo.") or k.startswith("algo."):
            algo_overrides[k] = v
            continue
        if k.startswith("cfgs.runner.") or k.startswith("runner."):
            runner_overrides[k] = v
            continue

    env_args = EnvArgsRegistry[env_name]
    env_args = apply_overrides_generic(env_args, env_overrides, prefixes=("cfgs.", "env."))
    algo_cfg = _apply_algo_overrides(PPO_WALKING_MLP, algo_overrides)
    runner_args = _apply_runner_overrides(RUNNER_WALKING_MLP, runner_overrides)

    if eval:
        # Evaluation mode - don't create runner to avoid creating empty log dir
        num_envs = 1
        print("Evaluation mode: Loading trained policy")
        evaluate_policy(
            exp_name=exp_name,
            show_viewer=show_viewer,
            num_ckpt=num_ckpt,
            device=device,
            env_args=env_args,
            algo_cfg=algo_cfg,
        )
    else:
        # Training mode
        print("Training mode: Starting policy training")
        train_policy(
            exp_name=exp_name,
            show_viewer=show_viewer,
            num_envs=num_envs,
            device=device,
            use_wandb=use_wandb,
            env_args=env_args,
            algo_cfg=algo_cfg,
            runner_args=runner_args,
        )


if __name__ == "__main__":
    fire.Fire(main)
